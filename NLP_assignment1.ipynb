{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNi4grZkK3xossgBvDb2FMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawkmak3r/NLP/blob/master/NLP_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SclSJB2bOkay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing pandas library for reading our csv file\n",
        "import pandas as pd\n",
        "#from sklearn we import a train_test_split which helps us to divide our data of\n",
        "#housing into 2 sets train and test model\n",
        "from sklearn.model_selection import train_test_split\n",
        "#importing numpy helps us to work with data and modify as per our need\n",
        "import numpy as np\n",
        "#for ploting our graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#imports require for our network\n",
        "import torch\n",
        "#under are different layers we need\n",
        "from torch.nn import Conv1d\n",
        "from torch.nn import MaxPool1d\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import  Linear\n",
        "from torch.nn.functional import relu\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjl3rcxvQxf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing dataset and remove any N/A entries \n",
        "data_set = pd.read_csv('/content/housing.csv')\n",
        "data_set = data_set.dropna()\n",
        "\n",
        "#perfroming visualization\n",
        "#as given in assignmet first we printed 10 records then we printed our plots \n",
        "#wwhich ar given in the assignment\n",
        "#first ten records from out dataset\n",
        "data_set.head(10)\n",
        "#plots \n",
        "DS=data_set[:17]\n",
        "plots=pd.DataFrame(DS,columns= DS.columns)\n",
        "plots.plot(subplots=True, figsize=(10, 10) )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cfac6e20-d01d-4e2c-ff00-a3c62446cded",
        "id": "CAvsRaA6donj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#for predicting medain house value column we define it as our Y variable\n",
        "Y = data_set['median_house_value']\n",
        "#for predicting we will use  longitude, latitude, housing median age, total \n",
        "#number of rooms, total number of bedrooms, population, number of households, \n",
        "#and median income. Defining those as X\n",
        "X = data_set.loc[:,'longitude':'median_income']\n",
        "print(X.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20433, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J7DZU7JSrt8",
        "colab_type": "code",
        "outputId": "a34c9a4d-449f-4e2a-b2b0-4e6cc53e82e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#spliting our data 70% for training and 30% for testing while random state is\n",
        "#set to 2003\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=2003)\n",
        "\n",
        "#to manipulate data we will convert it into numpy arrays \n",
        "#training data\n",
        "x_train_np = x_train.to_numpy()\n",
        "y_train_np = y_train.to_numpy()\n",
        "#test data\n",
        "x_test_np = x_test.to_numpy() \n",
        "y_test_np = y_test.to_numpy()\n",
        "print\n"
      ],
      "execution_count": 589,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function print>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 589
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo_TJattUYhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#definning our network for predication \n",
        "\n",
        "class Nlp_assignment1(torch.nn.Module):\n",
        "  def __init__(self, b_size, inputs, outputs):\n",
        "\n",
        "    super(Nlp_assignment1, self).__init__()\n",
        "    self.b_size = b_size\n",
        "    self.inputs = inputs\n",
        "    self.outputs = outputs \n",
        "\n",
        "    #first layer our input layer\n",
        "    self.input_layer = Conv1d(inputs, b_size, 1)\n",
        "    \n",
        "    #creating a max pooling layer\n",
        "    self.mpool_layer = MaxPool1d(1)\n",
        "\n",
        "    # Define another convolution layer\n",
        "    self.conv_layer = Conv1d(b_size , 128 , 1)\n",
        "\n",
        "    self.flatten_layer = Flatten()\n",
        "\n",
        "    self.lr_layer = Linear(128 , 64)\n",
        "    \n",
        "    #final a output layer\n",
        "    self.out_layer = Linear(64, outputs)\n",
        "\n",
        "    #All the layer are set\n",
        "    #creating feeding method to give our inputs to our layer \n",
        "\n",
        "  def feeder(self, input):\n",
        "    #as we using 1D convolution while our data is 3d we need to feed that data\n",
        "    #1d way\n",
        "    input = input.reshape((self.b_size, self.inputs, 1))\n",
        "\n",
        "    output = relu(self.input_layer(input))\n",
        "\n",
        "    output = self.mpool_layer(output)\n",
        "\n",
        "    output = relu(self.conv_layer(output))\n",
        "\n",
        "    output = self.flatten_layer(output)\n",
        "\n",
        "    output = self.lr_layer(output)\n",
        "\n",
        "    output = self.out_layer(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwoHCm1IdsDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#block of training our model\n",
        "\n",
        "from torch.optim import SGD\n",
        "from torch.optim import Adam\n",
        "from torch.nn import L1Loss\n",
        "from torchvision import models\n",
        "\n",
        "!pip install pytorch-ignite\n",
        "\n",
        "from ignite.contrib.metrics.regression.r2_score import R2Score\n",
        "\n",
        "\n",
        "b_size = 32\n",
        "\n",
        "model = Nlp_assignment1(b_size, X.shape[1], 1)\n",
        "\n",
        "print('The total number of trainable paramaeters are ',sum([param.nelement() for param in model.parameters()]))\n",
        "model.cuda()\n",
        "\n",
        "model = models.vgg16()\n",
        "print(model)\n",
        "print('as we can see from method our kernal size is 3 x 3 ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfZiQa-GED3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('0889960_1dconv_reg' ,'wb' ) as f:\n",
        "  pickle.dump(model,f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDGwhszthPK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for Average L1 loss and our R2 score \n",
        "\n",
        "def Avg_loss(model, data_set, train = False, optimizer = None):\n",
        "  perf = L1Loss()\n",
        "  score_r = R2Score()\n",
        "\n",
        "  avg_loss, avg_score, count = 0, 0, 0\n",
        "  for input, output in iter(data_set):\n",
        "\n",
        "    predict = model.feeder(input)\n",
        "\n",
        "    l = perf(predict, output)\n",
        "\n",
        "    score_r.update([predict, output])\n",
        "    score = score_r.compute()\n",
        "\n",
        "    if(train):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      l.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "    avg_loss += l.item()\n",
        "    avg_score += score\n",
        "    count += 1\n",
        "  return avg_loss / count, avg_score / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMX29tDCkOJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "\n",
        "# here we are using Adam becuase SGD fails to compute as per our requirement and\n",
        "#Adam provides us better output then SGD\n",
        "optimizer = Adam(model.parameters() , lr=1e-4)\n",
        "\n",
        "#Again as done before we are converting data in numpy arrary so we can manipulate\n",
        "#it as per our wish\n",
        "inputs = torch.from_numpy(x_test_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()\n",
        "#creating a Dataloader for loading our given data for batchs \n",
        "tensor = TensorDataset(inputs , outputs)\n",
        "loader = DataLoader(tensor, b_size, shuffle = True, drop_last = True)\n",
        "#Starting our timer for training model\n",
        "from time import perf_counter\n",
        "t1_start = perf_counter()  \n",
        "#intitaling our loop with given number of epochs \n",
        "for epoch in range(epochs): \n",
        "# going through the batches and get the average loss \n",
        "  avg_loss, avg_r2_score = Avg_loss(model, loader, train=True, optimizer=optimizer)\n",
        "# printing average loss per one epoch as we train though them\n",
        "  print(\"Epoch \" + str(epoch + 1) + \":\\n\\tLoss = \" + str(avg_loss) + \"\\n\\tR^2 Score = \" + str(avg_r2_score))\n",
        "\n",
        "inputs = torch.from_numpy(x_test_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0],1 )).cuda().float()\n",
        "\n",
        "tensor = TensorDataset(inputs, outputs) \n",
        "loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "avg_loss, avg_r2_score = Avg_loss(model, loader) \n",
        "print(\"this model has L1 loss: \" + str(avg_loss)) \n",
        "print(\"This model has a  R^2 score of \" + str(avg_r2_score))\n",
        "t1_stop = perf_counter() \n",
        "  \n",
        "print(\"Inference time in seconds \", t1_stop-t1_start)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Amw_ChI12R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}