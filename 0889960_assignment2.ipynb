{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0889960_assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawkmak3r/NLP/blob/master/0889960_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhZ-SKnmSUR-",
        "colab_type": "code",
        "outputId": "e5726ba8-fe2a-457c-e48e-d19fb6b229ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#libraries we need to compute this assignment\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import nltk \n",
        "import random \n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt') \n",
        "nltk.download('stopwords') \n",
        "nltk.download('wordnet') "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzgCW0E6SZQe",
        "colab_type": "code",
        "outputId": "f536f9dd-948a-4692-a1cd-e8376b560194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#importing for data into our system\n",
        "#please change your path according your use\n",
        "dataset = pd.read_csv(\"train.tsv\",delimiter='\\t',encoding='utf-8')\n",
        "dataset.head()\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDU5qDu_Sbj2",
        "colab_type": "code",
        "outputId": "4faf846e-f557-4a14-8728-9d943744fc88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#this return a random sample of items from an axis of object.\n",
        "dataset = dataset.sample(frac=1)\n",
        "#as we can see above we have 2 index which typically happens when dealing with\n",
        "dataset = dataset.reset_index(drop=True)\n",
        "dataset.head()\n",
        "#this way we are generating random"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65341</td>\n",
              "      <td>3308</td>\n",
              "      <td>is historical filmmaking without the balm of r...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49802</td>\n",
              "      <td>2442</td>\n",
              "      <td>the more outrageous bits achieve a shock-you-i...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>36259</td>\n",
              "      <td>1714</td>\n",
              "      <td>Give Shapiro , Goldman , and Bolado credit for...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16041</td>\n",
              "      <td>689</td>\n",
              "      <td>petty</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>91001</td>\n",
              "      <td>4734</td>\n",
              "      <td>top and movies</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0     65341  ...          3\n",
              "1     49802  ...          3\n",
              "2     36259  ...          1\n",
              "3     16041  ...          1\n",
              "4     91001  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09uWbN4SfNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#as per splitting our data 70 & 30 as per given in our assigment\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(dataset ['Phrase'], dataset ['Sentiment'], test_size=0.3)\n",
        "words=[]\n",
        "#passing our dataframe into list so we can manipulate it with easy\n",
        "X_train = np.array(X_train.values.tolist())\n",
        "Y_train = np.array(Y_train.values.tolist())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCZuUn7KKQ1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for training we need to tokenize our data \n",
        "#tokenize help us to train our data\n",
        "#which will be stored in words[] list\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  words.append([list(word_tokenize(X_train[i])), Y_train[i]]) \n",
        "X_test = np.array(X_test.values.tolist())\n",
        "Y_test = np.array(Y_test.values.tolist())\n",
        "for i in range(len(X_test)):\n",
        "  words.append([list(word_tokenize(X_test[i])), Y_test[i]]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ccNGN3WvQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now time to preprocess our data\n",
        "#here we are using proterStemmmer \n",
        "#then word lemmatizer for pre process\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer \n",
        "porter = PorterStemmer() \n",
        "lancaster=LancasterStemmer() \n",
        "wordnet_lemmatizer = WordNetLemmatizer() \n",
        "stopwords_en = stopwords.words(\"english\") \n",
        "punctuations=\"?:!.,;'\\\"-()\"\n",
        "remove_stopwords = True\n",
        "#as we have in build libraries for removing stopwords adn puncation\n",
        "#we are going to be using that to help us with better\n",
        "removePuncs = True\n",
        "useStemming = False\n",
        "useLemma = False\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wibkp1lHWxuf",
        "colab_type": "code",
        "outputId": "87368ecd-f8e1-44d2-deca-b386ad900ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#this loop will itrate over words and as per labels and starts removing \n",
        "#no neccasary stuff\n",
        "for l in range(len(words)):                    \n",
        "  label = words[l][1]                        \n",
        "  tmpReview = []                                  \n",
        "  for w in words[l][0]:                       \n",
        "    newWord = w                                   \n",
        "    if remove_stopwords and (w in stopwords_en):  \n",
        "      continue                                    \n",
        "    if removePuncs and (w in punctuations):       \n",
        "      continue                                    \n",
        "    if useStemming:\n",
        "      newWord = lancaster.stem(newWord) \n",
        "    if useLemma: \n",
        "      newWord = wordnet_lemmatizer.lemmatize(newWord) \n",
        "    tmpReview.append(newWord)                     \n",
        "  words[l] = (tmpReview, label)              \n",
        "  words[l] = (' '.join(tmpReview), label) \n",
        "\n",
        "print(words[0])\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('natural-seeming actors', 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InPVUfgzWztX",
        "colab_type": "code",
        "outputId": "abfe3f00-9a6f-4fdb-8482-61977128f3a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#ONCE WE OUR WORDS WE WILL BE ADDING IT AND WILL MAKE A DATASET\n",
        "#WITHOUT ANY repitation \n",
        "dataset = pd.DataFrame(words, columns=['text', 'sentiment']) \n",
        "dataset.head()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>natural-seeming actors</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gorgeous film must everyone junior scientists ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>canny derivative wildly gruesome</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cutesy film references</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>could confusing horrifying vision intense engr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0                             natural-seeming actors          3\n",
              "1  gorgeous film must everyone junior scientists ...          4\n",
              "2                   canny derivative wildly gruesome          3\n",
              "3                             cutesy film references          2\n",
              "4  could confusing horrifying vision intense engr...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSoEouQN83iF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(dataset['text'],  dataset['sentiment'], test_size=0.3, random_state=2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gaF2YPvUH7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
        "from keras.utils import to_categorical\n",
        "#AS we know for that our data is really big and hard to deal with\n",
        "#so as prof told we vectorizers our data \n",
        "vectorizer = TfidfVectorizer(max_features = 2000)\n",
        "X = vectorizer.fit_transform(dataset[\"text\"]) \n",
        "Y = dataset['sentiment'] \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(dataset['text'],  \n",
        "                                                    dataset['sentiment'], \n",
        "                                                    test_size=0.3) \n",
        "#once again we will split our clean data into 70 and 30 train and test\n",
        "X_train = vectorizer.transform(X_train).toarray()\n",
        "Y_train = Y_train \n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "Y_test = Y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezvrFic8Uqyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46f56d53-a254-4ae5-b0c6-c494a0655e90"
      },
      "source": [
        "#we are using keras we are going to train our data\n",
        "#importing libraries \n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5ToySXMU87V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converts a class vector integers to binary class matrix.\n",
        "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
        "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
        "#our model sequential conv1d model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA3I6AgtgMaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(lr =0.01 ),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rXqFVIQiS-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c4561401-4674-43f8-f461-12f3acbf3044",
        "id": "htjn0uz0AHM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          batch_size=64,\n",
        "          epochs=30)\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "109242/109242 [==============================] - 43s 398us/step - loss: 1.0078 - acc: 0.6011\n",
            "Epoch 2/30\n",
            "109242/109242 [==============================] - 43s 394us/step - loss: 0.9745 - acc: 0.6143\n",
            "Epoch 3/30\n",
            "109242/109242 [==============================] - 43s 395us/step - loss: 0.9661 - acc: 0.6193\n",
            "Epoch 4/30\n",
            "109242/109242 [==============================] - 43s 394us/step - loss: 0.9574 - acc: 0.6188\n",
            "Epoch 5/30\n",
            "109242/109242 [==============================] - 43s 394us/step - loss: 0.9446 - acc: 0.6239\n",
            "Epoch 6/30\n",
            "109242/109242 [==============================] - 43s 394us/step - loss: 0.9410 - acc: 0.6238\n",
            "Epoch 7/30\n",
            "109242/109242 [==============================] - 43s 394us/step - loss: 0.9346 - acc: 0.6274\n",
            "Epoch 8/30\n",
            "109242/109242 [==============================] - 43s 394us/step - loss: 0.9331 - acc: 0.6280\n",
            "Epoch 9/30\n",
            "109242/109242 [==============================] - 43s 397us/step - loss: 0.9329 - acc: 0.6281\n",
            "Epoch 10/30\n",
            "109242/109242 [==============================] - 43s 393us/step - loss: 0.9279 - acc: 0.6302\n",
            "Epoch 11/30\n",
            "109242/109242 [==============================] - 43s 396us/step - loss: 0.9235 - acc: 0.6312\n",
            "Epoch 12/30\n",
            "109242/109242 [==============================] - 44s 402us/step - loss: 0.9225 - acc: 0.6310\n",
            "Epoch 13/30\n",
            "109242/109242 [==============================] - 43s 396us/step - loss: 0.9226 - acc: 0.6321\n",
            "Epoch 14/30\n",
            "109242/109242 [==============================] - 43s 397us/step - loss: 0.9191 - acc: 0.6338\n",
            "Epoch 15/30\n",
            "109242/109242 [==============================] - 44s 400us/step - loss: 0.9156 - acc: 0.6360\n",
            "Epoch 16/30\n",
            "109242/109242 [==============================] - 44s 400us/step - loss: 0.9204 - acc: 0.6322\n",
            "Epoch 17/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.9128 - acc: 0.6372\n",
            "Epoch 18/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.9122 - acc: 0.6362\n",
            "Epoch 19/30\n",
            "109242/109242 [==============================] - 44s 400us/step - loss: 0.9116 - acc: 0.6369\n",
            "Epoch 20/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.9186 - acc: 0.6336\n",
            "Epoch 21/30\n",
            "109242/109242 [==============================] - 44s 400us/step - loss: 0.9060 - acc: 0.6396\n",
            "Epoch 22/30\n",
            "109242/109242 [==============================] - 44s 402us/step - loss: 0.9134 - acc: 0.6362\n",
            "Epoch 23/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.9074 - acc: 0.6375\n",
            "Epoch 24/30\n",
            "109242/109242 [==============================] - 44s 402us/step - loss: 0.9043 - acc: 0.6389\n",
            "Epoch 25/30\n",
            "109242/109242 [==============================] - 44s 402us/step - loss: 0.9217 - acc: 0.6315\n",
            "Epoch 26/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.9038 - acc: 0.6396\n",
            "Epoch 27/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.9001 - acc: 0.6409\n",
            "Epoch 28/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.9074 - acc: 0.6381\n",
            "Epoch 29/30\n",
            "109242/109242 [==============================] - 44s 402us/step - loss: 0.9028 - acc: 0.6391\n",
            "Epoch 30/30\n",
            "109242/109242 [==============================] - 44s 401us/step - loss: 0.8988 - acc: 0.6416\n",
            "Test loss: 1.0856135156623241\n",
            "Test accuracy: 0.6000683497799991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYtR-K_whiGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('0889960_sentiment_Analyaze' ,'wb' ) as f:\n",
        "  pickle.dump(model,f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}